[[introduction]]
= Observability

Spring AI builds upon the observability features in the Spring ecosystem to provide insights into AI-related operations.
Spring AI provides metrics and tracing capabilities for its core components: `ChatClient` (including `Advisor`),
`ChatModel`, `EmbeddingModel`, `ImageModel`, and `VectorStore`.

NOTE: Low cardinality keys will be added to metrics and traces, while high cardinality keys will only be added to traces.

== Chat Client

The `spring.ai.chat.client` observations are recorded when a ChatClient `call()` or `stream()` operations are invoked. 
They measure the time spent performing the invocation and propagate the related tracing information.

.Low Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.operation.name` | Always `framework`.
|`gen_ai.system` | Always `spring_ai`.
|`spring.ai.chat.client.stream` | Is the chat model response a stream - `true or false`
|`spring.ai.kind` | The kind of framework API in Spring AI: `chat_client`.
|===

.High Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`spring.ai.chat.client.advisor.params` | Map of advisor parameters.
|`spring.ai.chat.client.advisors` | List of configured chat client advisors.
|`spring.ai.chat.client.system.params` |Chat client system parameters. Optional.
|`spring.ai.chat.client.system.text` |Chat client system text. Optional.
|`spring.ai.chat.client.tool.function.names` | Enabled tool function names.
|`spring.ai.chat.client.tool.function.callbacks` |List of configured chat client function callbacks.
|`spring.ai.chat.client.user.params` | Chat client user parameters. Optional.
|`spring.ai.chat.client.user.text` | Chat client user text. Optional.
|===

=== Input Data

The `ChatClient` input data is typically big and possibly containing sensitive information.
For those reasons, it is not exported by default.

Spring AI supports exporting input data as span attributes across all tracing backends.

[cols="6,3,1", stripes=even]
|====
| Property | Description | Default

| `spring.ai.chat.client.observations.include-input` |  Whether to include the input content in the observations. | `false`
|====

WARNING: If you enable the inclusion of the input content in the observations, there's a risk of exposing sensitive or private information. Please, be careful!

=== Chat Client Advisors

The `spring.ai.advisor` observations are recorded when a call or stream around advisors is performed. 
They measure the time spent in the advisor (including the time spend on the inner advisors) and propagate the related tracing information.

.Low Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.operation.name` | Always `framework`.
|`gen_ai.system` | Always `spring_ai`.
|`spring.ai.advisor.type` | Where the advisor applies it's logic in the request processing, one of `BEFORE`, `AFTER`, or `AROUND`.
|`spring.ai.kind` | The kind of framework API in Spring AI: `advisor`.
|===

.High Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`spring.ai.advisor.name`| Name of the advisor.
|`spring.ai.advisor.order`| Advisor order in the advisor chain.
|===

== Chat Model

NOTE: Observability features are currently supported only for `ChatModel` implementations from the following AI model
providers: Anthropic, Azure OpenAI, Mistral AI, Ollama, OpenAI, Vertex AI, MiniMax, Moonshot, QianFan, Zhiu AI.
Additional AI model providers will be supported in a future release.

The `gen_ai.client.operation` observations are recorded when calling the ChatModel `call` or `stream` methods. 
They measure the time spent on method completion and propagate the related tracing information.

IMPORTANT: The `gen_ai.client.token.usage` metrics measures number of input and output tokens used by a single model call.


.Low Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.operation.name` | The name of the operation being performed.
|`gen_ai.system` | The model provider as identified by the client instrumentation.
|`gen_ai.request.model` | The name of the model a request is being made to.
|`gen_ai.response.model` | The name of the model that generated the response.
|===

.High Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.request.frequency_penalty` | The frequency penalty setting for the model request.
|`gen_ai.request.max_tokens` | The maximum number of tokens the model generates for a request.
|`gen_ai.request.presence_penalty` | The presence penalty setting for the model request.
|`gen_ai.request.stop_sequences` | List of sequences that the model will use to stop generating further tokens.
|`gen_ai.request.temperature` | The temperature setting for the model request.
|`gen_ai.request.top_k` | The top_k sampling setting for the model request.
|`gen_ai.request.top_p` | The top_p sampling setting for the model request.
|`gen_ai.response.finish_reasons` | Reasons the model stopped generating tokens, corresponding to each generation received.
|`gen_ai.response.id` | The unique identifier for the AI response.
|`gen_ai.usage.input_tokens` | The number of tokens used in the model input (prompt).
|`gen_ai.usage.output_tokens` | The number of tokens used in the model output (completion).
|`gen_ai.usage.total_tokens` | The total number of tokens used in the model exchange.
|`gen_ai.prompt` | The full prompt sent to the model. Optional.
|`gen_ai.completion` | The full response received from the model. Optional.
|===

NOTE: For measuring user tokens, the previous table lists the values present in an observation trace.
Use the metric name `gen_ai.client.token.usage` that is provided by the `ChatModel`.

.Events
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.content.prompt` | Event including the content of the chat prompt. Optional.
|`gen_ai.content.completion` | Event including the content of the chat completion. Optional.
|===

=== Chat Prompt and Completion Data

The chat prompt and completion data is typically big and possibly containing sensitive information.
For those reasons, it is not exported by default.

Spring AI supports exporting chat prompt and completion data as span events if you use an OpenTelemetry tracing backend,
whereas data is exported as span attributes if you use an OpenZipkin tracing backend.

Furthermore, Spring AI supports logging chat prompt and completion data, useful for troubleshooting scenarios.

[cols="6,3,1", stripes=even]
|====
| Property | Description | Default

| `spring.ai.chat.observations.include-prompt` | Include the prompt content in observations. `true` or `false` | `false`
| `spring.ai.chat.observations.include-completion` | Include the completion content in observations. `true` or `false` | `false`
| `spring.ai.chat.observations.include-error-logging` | Include error logging in observations. `true` or `false` | `false`
|====

WARNING: If you enable the inclusion of the chat prompt and completion data in the observations, there's a risk of exposing sensitive or private information. Please, be careful!

== EmbeddingModel

NOTE: Observability features are currently supported only for `EmbeddingModel` implementations from the following
AI model providers: Azure OpenAI, Mistral AI, Ollama, and OpenAI.
Additional AI model providers will be supported in a future release.

The `gen_ai.client.operation` observations are recorded on embedding model method calls. 
They measure the time spent on method completion and propagate the related tracing information.

IMPORTANT: The `gen_ai.client.token.usage` metrics measures number of input and output tokens used by a single model call.

.Low Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.operation.name` | The name of the operation being performed.
|`gen_ai.system` | The model provider as identified by the client instrumentation.
|`gen_ai.request.model` | The name of the model a request is being made to.
|`gen_ai.response.model` | The name of the model that generated the response.
|===

.High Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.request.embedding.dimensions` | The number of dimensions the resulting output embeddings have.
|`gen_ai.usage.input_tokens` | The number of tokens used in the model input.
|`gen_ai.usage.total_tokens` | The total number of tokens used in the model exchange.
|===

NOTE: For measuring user tokens, the previous table lists the values present in an observation trace.
Use the metric name `gen_ai.client.token.usage` that is provided by the `EmbeddingModel`.

== Image Model

NOTE: Observability features are currently supported only for `ImageModel` implementations from the following AI model
providers: OpenAI.
Additional AI model providers will be supported in a future release.

The `gen_ai.client.operation` observations are recorded on image model method calls. 
They measure the time spent on method completion and propagate the related tracing information.

IMPORTANT: The `gen_ai.client.token.usage` metrics measures number of input and output tokens used by a single model call.


.Low Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.operation.name`| The name of the operation being performed.
|`gen_ai.system`| The model provider as identified by the client instrumentation.
|`gen_ai.request.model`| The name of the model a request is being made to.
|===

.High Cardinality Keys
|===
|Name | Description

|`gen_ai.request.image.response_format` | The format in which the generated image is returned.
|`gen_ai.request.image.size` | The size of the image to generate.
|`gen_ai.request.image.style` | The style of the image to generate.
|`gen_ai.response.id` | The unique identifier for the AI response.
|`gen_ai.response.model` | The name of the model that generated the response.
|`gen_ai.usage.input_tokens` | The number of tokens used in the model input (prompt).
|`gen_ai.usage.output_tokens` | The number of tokens used in the model output (generation).
|`gen_ai.usage.total_tokens` | The total number of tokens used in the model exchange.
|`gen_ai.prompt` | The full prompt sent to the model. Optional.
|===

NOTE: For measuring user tokens, the previous table lists the values present in an observation trace.
Use the metric name `gen_ai.client.token.usage` that is provided by the `ImageModel`.

.Events
[cols="a,a", stripes=even]
|===
|Name | Description

|`gen_ai.content.prompt` | Event including the content of the image prompt. Optional.
|===

=== Image Prompt Data

The image prompt data is typically big and possibly containing sensitive information.
For those reasons, it is not exported by default.

Spring AI supports exporting image prompt data as span events if you use an OpenTelemetry tracing backend,
whereas data is exported as span attributes if you use an OpenZipkin tracing backend.

[cols="6,3,1", stripes=even]
|===
| Property | Description | Default

| `spring.ai.image.observations.include-prompt` | `true` or `false` | `false`
|===

WARNING: If you enable the inclusion of the image prompt data in the observations, there's a risk of exposing sensitive or private information. Please, be careful!

== Vector Stores

All vector store implementations in Spring AI are instrumented to provide metrics and distributed tracing data through Micrometer.

The `db.vector.client.operation` observations are recorded when interacting with the Vector Store. 
They measure the time spent on the `query`, `add` and `remove` operations and propagate the related tracing information.

.Low Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`db.operation.name` | The name of the operation or command being executed. One of `add`, `delete`, or `query`.
|`db.system` | The database management system (DBMS) product as identified by the client instrumentation. One of `pg_vector`, `azure`, `cassandra`, `chroma`, `elasticsearch`, `milvus`, `neo4j`, `opensearch`, `qdrant`, `redis`, `typesense`, `weaviate`, `pinecone`, `oracle`, `mongodb`, `gemfire`, `hana`, `simple`.
|`spring.ai.kind` | The kind of framework API in Spring AI: `vector_store`.
|===

.High Cardinality Keys
[cols="a,a", stripes=even]
|===
|Name | Description

|`db.collection.name` | The name of a collection (table, container) within the database.
|`db.namespace` | The name of the database, fully qualified within the server address and port.
|`db.record.id` | The record identifier if present.
|`db.search.similarity_metric` | The metric used in similarity search.
|`db.vector.dimension_count` | The dimension of the vector.
|`db.vector.field_name` | The name field as of the vector (e.g. a field name).
|`db.vector.query.content` | The content of the search query being executed.
|`db.vector.query.filter` | The metadata filters used in the search query.
|`db.vector.query.response.documents` | Returned documents from a similarity search query. Optional.
|`db.vector.query.similarity_threshold` | Similarity threshold that accepts all search scores. A threshold value of 0.0 means any similarity is accepted or disable the similarity threshold filtering. A threshold value of 1.0 means an exact match is required.
|`db.vector.query.top_k` | The top-k most similar vectors returned by a query.
|===

.Events
[cols="a,a", stripes=even]
|===
|Name | Description

|`db.vector.content.query.response` | Event including the vector search response data. Optional.
|===

=== Response Data

The vector search response data is typically big and possibly containing sensitive information.
For those reasons, it is not exported by default.

Spring AI supports exporting vector search response data as span events if you use an OpenTelemetry tracing backend,
whereas data is exported as span attributes if you use an OpenZipkin tracing backend.

[cols="6,3,1", stripes=even]
|===
| Property | Description | Default

| `spring.ai.vectorstore.observations.include-query-response` | `true` or `false` | `false`
|===

WARNING: If you enable the inclusion of the vector search response data in the observations, there's a risk of exposing sensitive or private information. Please, be careful!

== Example: Sending traces to an OpenTelemetry Backend

This guide shows how to send traces to https://langfuse.com/[Langfuse]'s OpenTelemetry endpoint.

[NOTE]
====
Visit the https://github.com/langfuse/langfuse-examples/tree/main/applications/spring-ai-demo[Langfuse Example Repo] for a fully instrumented example application.
====

=== Step 1: Enable OpenTelemetry in Spring AI

*Add OpenTelemetry and Spring Observability Dependencies* (Maven):

Make sure your project includes Spring Boot Actuator and the Micrometer tracing libraries for OpenTelemetry. Spring Boot Actuator is required to enable Micrometer's observation and tracing features.

You'll also need the Micrometer -> OpenTelemetry bridge and an OTLP exporter. For Maven, add the following to your `pom.xml` (Gradle users can include equivalent coordinates in Gradle):

[source,xml]
----
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>io.opentelemetry.instrumentation</groupId>
            <artifactId>opentelemetry-instrumentation-bom</artifactId>
            <version>2.13.2</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>

<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-openai-spring-boot-starter</artifactId>
        <version>1.0.0-M6</version>
    </dependency>
    <!-- Spring AI needs a reactive web server to run for some reason-->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <dependency>
        <groupId>io.opentelemetry.instrumentation</groupId>
        <artifactId>opentelemetry-spring-boot-starter</artifactId>
    </dependency>
    <!-- Spring Boot Actuator for observability support -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>
    <!-- Micrometer Observation -> OpenTelemetry bridge -->
    <dependency>
        <groupId>io.micrometer</groupId>
        <artifactId>micrometer-tracing-bridge-otel</artifactId>
    </dependency>
    <!-- OpenTelemetry OTLP exporter for traces -->
    <dependency>
        <groupId>io.opentelemetry</groupId>
        <artifactId>opentelemetry-exporter-otlp</artifactId>
    </dependency>
</dependencies>
----

*Enable Span Export and Configure Spring AI Observations* (`application.yml`):

With the above dependencies, Spring Boot will auto-configure tracing using OpenTelemetry as long as we provide the proper settings. We need to specify where to send the spans (the OTLP endpoint) and ensure Spring AI is set up to include the desired data in those spans. Create or update your `application.yml` (or `application.properties`) with the following configurations:

[source,yaml]
----
spring:
  application:
    name: spring-ai-llm-app # Service name for tracing (appears in Langfuse UI as the source service)
  ai:
    chat:
      observations:
        include-prompt: true # Include prompt content in tracing (disabled by default for privacy)
        include-completion: true # Include completion content in tracing (disabled by default)
management:
  tracing:
    sampling:
      probability: 1.0 # Sample 100% of requests for full tracing (adjust in production as needed)
  observations:
    annotations:
      enabled: true # Enable @Observed (if you use observation annotations in code)
----

With these configurations and dependencies in place, your Spring Boot application is ready to produce OpenTelemetry traces. Spring AI's internal calls (e.g. when you invoke a chat model or generate an embedding) will be recorded as spans.

Each span will carry attributes like `gen_ai.operation.name`, `gen_ai.system` (the provider, e.g. "openai"), model names, token usage, etc., and – since we enabled them – events for the prompt and response content​

=== Step 2: Configure Langfuse

Now that your Spring AI application is emitting OpenTelemetry trace data, the next step is to direct that data to Langfuse.

Langfuse will act as the "backend" for OpenTelemetry in this setup – essentially replacing a typical Jaeger/Zipkin/OTel-Collector with Langfuse's trace ingestion API.

*Langfuse Setup*

- Sign up for https://cloud.langfuse.com/[Langfuse Cloud] or https://langfuse.com/self-hosting[self-hosted Langfuse].
- Set the OTLP endpoint (e.g. `https://cloud.langfuse.com/api/public/otel`) and API keys.

Configure these via environment variables:

[source,bash]
----
OTEL_EXPORTER_OTLP_ENDPOINT: set this to the Langfuse OTLP URL (e.g. https://cloud.langfuse.com/api/public/otel).
OTEL_EXPORTER_OTLP_HEADERS: set this to Authorization=Basic <base64 public:secret>.
----

[NOTE]
====
You can find more on authentication via Basic Auth https://langfuse.com/docs/opentelemetry/get-started[here].
====

=== Step 3: Run a Test AI Operation

Start your Spring Boot application. Trigger an AI operation that Spring AI handles – for example, call a service or controller that uses a `ChatModel` to generate a completion, or an `EmbeddingModel` to generate embeddings.

[source,java]
----
@Autowired
private ChatService chatService;

@EventListener(ApplicationReadyEvent.class)
public void testAiCall() {
    String answer = chatService.chat("Hello, Spring AI!");
    System.out.println("AI answered: " + answer);
}
----
